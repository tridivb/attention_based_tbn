{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from core.dataset import Rescale, TransferTensorDict, EpicClasses\n",
    "from core.tools import initialize\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Model initialized with imagenet weights\n",
      "Freezing the batchnorms of Base Model RGB except first or new layers.\n",
      "Model initialized with imagenet weights\n",
      "Freezing the batchnorms of Base Model Audio except first or new layers.\n",
      "Model initialized.\n",
      "----------------------------------------------------------\n",
      "Loading pre-trained weights /media/data/tridiv/epic/tbn_weights/attention/seen/epic_tbn_bninception_RGB_Audio_best.pth...\n",
      "Done.\n",
      "----------------------------------------------------------\n",
      "Reading list of test videos...\n",
      "Done.\n",
      "----------------------------------------------------------\n",
      "Creating the dataset using annotations/EPIC_train_action_labels.csv...\n",
      "Done.\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cfg, model, dataset, device = initialize(\"config/config_vis.yaml\")\n",
    "epic_classes = EpicClasses(os.path.join(cfg.data_dir, \"annotations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(yaxis=dict(range=[0, 1]))\n",
    "\n",
    "def visualize(model, dataset, index, device):\n",
    "    dict_to_device = TransferTensorDict(device)\n",
    "    data, target, _ = default_collate([dataset[index-1]])\n",
    "    rgb_indices = data[\"indices\"][\"RGB\"].numpy().squeeze()\n",
    "    data = dict_to_device(data)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "#     gt_weights = target[\"weights\"].numpy().squeeze(0)\n",
    "    weights = out[\"weights\"].cpu().numpy()\n",
    "    wts = []\n",
    "    fig = make_subplots(rows=2, cols=3)\n",
    "    for idx in range(weights.shape[0]):\n",
    "        x = np.arange(weights.shape[2])\n",
    "        wts = go.Scatter(x=x, y=weights[idx].squeeze(0))\n",
    "        img = Image.open(os.path.join(cfg.data_dir, cfg.data.rgb.dir_prefix, data[\"vid_id\"][0], \"img_{:010d}.jpg\".format(rgb_indices[idx])))\n",
    "        img = img.resize((128,128))\n",
    "        h, w = img.size\n",
    "        fig.add_trace(go.Scatter(x=[0, w], y=[0, h], mode='markers'), row=1, col=idx+1)\n",
    "        fig.add_layout_image(\n",
    "            dict(\n",
    "                source=img,\n",
    "                x=40,\n",
    "                y=128,\n",
    "                sizex=w,\n",
    "                sizey=h,\n",
    "                opacity=1,\n",
    "                layer=\"above\"),\n",
    "            row=1, col=idx+1\n",
    "        )\n",
    "        fig.add_trace(wts, row=2, col=idx+1)\n",
    "        fig.update_yaxes(range=[0, 1], row=2, col=idx+1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3af40778294b9ab10c6d455a88c7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='index', max=2398, min=1), Output()), _dom_classes=('widgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize(model, dataset, index, device)>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(visualize, model=fixed(model), dataset=fixed(dataset), index=widgets.IntSlider(min=1, max=len(dataset), step=1, value=0), device=fixed(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rgb = np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]],\n",
    "                    [[0, 255, 0], [0, 0, 255], [255, 0, 0]]\n",
    "                   ], dtype=np.uint8)\n",
    "img_rgb.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
