{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from core.dataset import Rescale, TransferTensorDict, EpicClasses\n",
    "from core.tools import initialize, visualize\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Model initialized with imagenet weights\n",
      "Freezing the batchnorms of Base Model RGB except first or new layers.\n",
      "Model initialized with imagenet weights\n",
      "Freezing the batchnorms of Base Model Audio except first or new layers.\n",
      "Model initialized.\n",
      "----------------------------------------------------------\n",
      "Loading pre-trained weights /media/data/tridiv/epic/tbn_weights/prior_kl/unseen/epic_tbn_bninception_RGB_Audio.pth...\n",
      "Done.\n",
      "----------------------------------------------------------\n",
      "Reading list of test videos...\n",
      "Done.\n",
      "----------------------------------------------------------\n",
      "Creating the dataset using annotations/EPIC_train_action_labels.csv...\n",
      "Done.\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cfg, model, dataset, epic_classes, device = initialize(\"config/config_vis.yaml\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=24,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca395d98814748b6b38647a43ed525ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='index', max=5793, min=1), Outpuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function core.tools.vis.visualize(cfg, model, dataset, index, epic_classes, device)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "interact(visualize, cfg=fixed(cfg), model=fixed(model), dataset=fixed(dataset), index=widgets.IntSlider(min=1, max=len(dataset), step=1, value=0, continuous_update=False), epic_classes=fixed(epic_classes), device=fixed(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0311, 0.0260, 0.0515, 0.0460, 0.0475, 0.0212, 0.0491, 0.0459, 0.0530,\n",
       "         0.0239, 0.0329, 0.0373, 0.0429, 0.0481, 0.0538, 0.0319, 0.0234, 0.0407,\n",
       "         0.0385, 0.0467, 0.0263, 0.0425, 0.0457, 0.0539, 0.0404]),\n",
       " tensor([0.0000, 0.0000, 0.0515, 0.0460, 0.0475, 0.0000, 0.0491, 0.0459, 0.0530,\n",
       "         0.0000, 0.0000, 0.0000, 0.0429, 0.0481, 0.0538, 0.0000, 0.0000, 0.0407,\n",
       "         0.0000, 0.0467, 0.0000, 0.0425, 0.0457, 0.0539, 0.0404]),\n",
       " tensor([ 0.0311,  0.0260, -0.0515, -0.0460, -0.0475,  0.0212, -0.0491, -0.0459,\n",
       "         -0.0530,  0.0239,  0.0329,  0.0373, -0.0429, -0.0481, -0.0538,  0.0319,\n",
       "          0.0234, -0.0407,  0.0385, -0.0467,  0.0263, -0.0425, -0.0457, -0.0539,\n",
       "         -0.0404]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropies = torch.stack(entropy_list)\n",
    "# sorted_indices = entropies.topk(entropies.shape[0]).indices\n",
    "# sorted_indices\n",
    "a = torch.softmax(torch.rand(25), dim=0)\n",
    "m = a.clone().detach()\n",
    "m[m >= 0.04] = 1\n",
    "m[m < 0.04] = 0\n",
    "l = (a * (1 - m)) - (a * m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
